{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcOM2Wge4hZU",
        "outputId": "4a7af20c-7c3c-44a0-96b4-9e428d8e1f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract key insights from the story\n",
        "\n",
        "def extract_key_insights(text):\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "\n",
        "    # phrases for important insights\n",
        "    importance_markers = [\n",
        "        'most effective', 'best way', 'key', 'instead of',\n",
        "        'should focus', 'need to', 'have to'\n",
        "    ]\n",
        "\n",
        "    scored_sentences = []\n",
        "\n",
        "\n",
        "    # giving scores to the sentence\n",
        "    for sentence in sentences:\n",
        "        score = 0\n",
        "        lower_sent = sentence.lower()\n",
        "\n",
        "        # score - based on importance markers\n",
        "        for marker in importance_markers:\n",
        "            if marker in lower_sent:\n",
        "                score += 2\n",
        "\n",
        "        # score - is sentence based on learning something?\n",
        "        if any(word in lower_sent for word in ['learn', 'remember', 'practice', 'active', 'focus']):\n",
        "            score += 1\n",
        "\n",
        "        # score - should you do it?\n",
        "        if any(word in lower_sent for word in ['should', 'need', 'must', 'have to', 'can']):\n",
        "            score += 1\n",
        "\n",
        "        # score - is it teaching you anything?\n",
        "        if any(word in lower_sent for word in ['instead of', 'rather than', 'solution']):\n",
        "            score += 2\n",
        "\n",
        "        if score > 0:\n",
        "            scored_sentences.append((sentence, score))\n",
        "\n",
        "\n",
        "    # sort by score\n",
        "    scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "    # other unique insights\n",
        "    filtered_insights = []\n",
        "    used_words = set()\n",
        "\n",
        "    for sentence, score in scored_sentences:\n",
        "\n",
        "        # if words are unique then consider the word\n",
        "        words = set(tokenizer.tokenize(sentence.lower()))\n",
        "        if len(words.intersection(used_words)) / len(words) < 0.5:\n",
        "            filtered_insights.append(sentence)\n",
        "            used_words.update(words)\n",
        "\n",
        "        if len(filtered_insights) >= 5:\n",
        "            break\n",
        "\n",
        "\n",
        "    return filtered_insights"
      ],
      "metadata": {
        "id": "g8cgxk1h4kr3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output text format\n",
        "\n",
        "def format_insights(insights):\n",
        "    print(\"\\nKey Learning Insights from the Text:\\n\")\n",
        "    for i, insight in enumerate(insights, 1):\n",
        "        cleaned_insight = insight.strip().replace('\\n', ' ')\n",
        "        print(f\"{i}. {cleaned_insight}\")"
      ],
      "metadata": {
        "id": "6-DxB8rt4oC6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# give your story here\n",
        "\n",
        "story = \"\"\"\n",
        "The Best Way To Remember Almost Everything You Learn. We learn how to cook. We learn an effective way to deal with people in the workplace. We learn a good way to maintain a good relationship with our family. There are so many things to learn, yet we have limited time.\n",
        "In fact, the best way of learning is not just spending more time.\n",
        "In this article, I will show you the most effective way to remember almost everything we learn.\n",
        "Learning Pyramid: The Key Connection Between our Brain and Learning\n",
        "In 1960s, NTL Institute published a research on how humans learn things.\n",
        "Instead of forcing ourselves to remember more information with the traditional passive methods, we should focus on the active ways to learn things.\n",
        "Apparently, we all have learned things from the active methods. We don't pay attention at all. Look at these examples:\n",
        "If a 2 years old boy wants to learn how to walk, he needs to practice how to walk instead of watching how others walk.\n",
        "If we want to learn a new foreign language, we should focus on speaking with other native speakers.\n",
        "If we want to get fit, we have to work with other people or a personal trainer instead of watching videos\n",
        "Sounds familiar, right?\n",
        "\"I don't have time…\"\n",
        "One of the most common excuses for ourselves to say, \"I don't have time…\"\n",
        "Unfortunately, I always make this excuse, too. I feel guilty.\n",
        "There are 24 hours in a day (or 23 hours, 56 minutes, and 4.1 seconds). We all have the same amount of time in each day. Ev Williams, Larry Kim, Arianna Huffington, and other millionaires have the same amount of time as we all do. Yet, they have built the most successful businesses in the world.\n",
        "All successful millionaires started from nothing. They learned how to maximize time for effectiveness.\n",
        "If I watch a 5-minutes YouTube tutorial of \"how to calm an aggressive cat\" instead of a 15-minutes video tutorial with the similar content, doing simple math, I have more time to learn more different thing and retain the same amount of information.\n",
        "We have to make the most out of the limited time by focusing on the solutions. We need to learn to reject everything else that can distract us from our goal.\n",
        "If we want to remember more new information everything, we need to spend less time on re-learning what we have learned already, and focus on the new ones.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Dkq1o-jJ4rE4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the insights\n",
        "\n",
        "insights = extract_key_insights(story)\n",
        "format_insights(insights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6EQ1bWg4tcE",
        "outputId": "e4d6bb1b-bc30-44ab-9176-f5c28cb3101e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Key Insights from the Text:\n",
            "\n",
            "1. Instead of forcing ourselves to remember more information with the traditional passive methods, we should focus on the active ways to learn things.\n",
            "2. If we want to get fit, we have to work with other people or a personal trainer instead of watching videos Sounds familiar, right?\n",
            "3. Look at these examples: If a 2 years old boy wants to learn how to walk, he needs to practice how to walk instead of watching how others walk.\n",
            "4. If I watch a 5-minutes YouTube tutorial of \"how to calm an aggressive cat\" instead of a 15-minutes video tutorial with the similar content, doing simple math, I have more time to learn more different thing and retain the same amount of information.\n",
            "5. We need to learn to reject everything else that can distract us from our goal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQFrXrAw4vN1"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}